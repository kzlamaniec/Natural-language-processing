# załadowanie bibliotek
library(tm)
library(hunspell)
# utworzenie korpusu dokumentów
corpus_dir <- "./20_Dokumenty"
corpus <- VCorpus(
DirSource(
corpus_dir,
"UTF-8",
"*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
# załadowanie bibliotek
library(tm)
library(hunspell)
# utworzenie korpusu dokumentów
corpus_dir <- "./20_Dokumenty"
corpus <- VCorpus(
DirSource(
corpus_dir,
"UTF-8",
"*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
# dodatkowe funkcje transformujące
paste_paragraphs <- content_transformer(
function(text){
paste(text, collapse = " ")
}
)
remove_char <- function(text, char) gsub(char, "", text)
cut_extension <- function(document){
meta(document, "id") <- gsub("\\.txt$", "", meta(document, "id"))
return(document)
}
# wstępne przetwarzanie
corpus <- tm_map(corpus, cut_extension)
corpus <- tm_map(corpus, paste_paragraphs)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
stoplist_file <- "./stopwords_pl.txt"
stoplist <- readLines(
stoplist_file,
encoding = "utf-8"
)
corpus <- tm_map(corpus, removeWords, stoplist)
corpus <- tm_map(corpus, content_transformer(remove_char), intToUtf8(8722))
corpus <- tm_map(corpus, content_transformer(remove_char), intToUtf8(190))
corpus <- tm_map(corpus, content_transformer(trimws))
corpus <- tm_map(corpus, stripWhitespace)
# lematyzacja
polish <- dictionary("pl_PL")
# lematyzacja
polish <- dictionary("pl_PL")
lemmatize <- function(text){
parsed_text_vec <- unlist(hunspell_parse(text, dict = polish))
lemmatized_text_vec <- hunspell_stem(parsed_text_vec, dict = polish)
for (t in 1:length(lemmatized_text_vec)) {
if(length(lemmatized_text_vec[[t]]) == 0) lemmatized_text_vec[t] <- parsed_text_vec[t]
if(length(lemmatized_text_vec[[t]])  > 1) lemmatized_text_vec[t] <- lemmatized_text_vec[[t]][1]
}
lemmatized_text <- paste(lemmatized_text_vec, collapse = " ")
return(lemmatized_text)
}
corpus <- tm_map(corpus, content_transformer(lemmatize))
# lematyzacja
polish <- dictionary("pl_PL")
lemmatize <- function(text){
parsed_text_vec <- unlist(hunspell_parse(text, dict = polish))
lemmatized_text_vec <- hunspell_stem(parsed_text_vec, dict = polish)
for (t in 1:length(lemmatized_text_vec)) {
if(length(lemmatized_text_vec[[t]]) == 0) lemmatized_text_vec[t] <- parsed_text_vec[t]
if(length(lemmatized_text_vec[[t]])  > 1) lemmatized_text_vec[t] <- lemmatized_text_vec[[t]][1]
}
lemmatized_text <- paste(lemmatized_text_vec, collapse = " ")
return(lemmatized_text)
}
corpus <- tm_map(corpus, content_transformer(lemmatize))
# eksport przetworzonego korpusu do plików
preprocessed_dir <- "./przetworzone"
dir.create(preprocessed_dir)
writeCorpus(corpus,preprocessed_dir)
corpus_dir <- "./przetworzone"
corpus_dir <- "./przetworzone"
corpus <- VCorpus(
DirSource(
corpus_dir,
"UTF-8",
"*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
# dodatkowe funkcje transformujące
cut_extension <- function(document){
meta(document, "id") <- gsub("\\.txt$", "", meta(document, "id"))
return(document)
}
# wstępne przetwarzanie
corpus <- tm_map(corpus, cut_extension)
# tworzenie macierzy częstości
tdm_tf_all <- TermDocumentMatrix(corpus)
# transponowana tdm
dtm_tf_all <- DocumentTermMatrix(corpus)
tdm_tfidf_216 <- DocumentTermMatrix(
corpus,
control = list(
weighting = weightTfIdf,
bounds = list(
global = c(2,16)
)
)
)
tdm_bin_410 <- DocumentTermMatrix(
corpus,
control = list(
weighting = weightBin,
bounds = list(
global = c(4,10)
)
)
)
tdm_tf_612 <- DocumentTermMatrix(
corpus,
control = list(
weighting = weightTf,
bounds = list(
global = c(6,12)
)
)
)
tdm_tfidf_216_m <- as.matrix(tdm_tfidf_216)
tdm_bin_410_m <- as.matrix(tdm_bin_410)
tdm_tf_612_m <- as.matrix(tdm_tf_612)
# załadowanie bibliotek
library(tm)
library(hunspell)
# utworzenie korpusu dokumentów
corpus_dir <- "./20_Dokumenty"
corpus <- VCorpus(
DirSource(
corpus_dir,
"UTF-8",
"*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
# dodatkowe funkcje transformujące
paste_paragraphs <- content_transformer(
function(text){
paste(text, collapse = " ")
}
)
remove_char <- function(text, char) gsub(char, "", text)
cut_extension <- function(document){
meta(document, "id") <- gsub("\\.txt$", "", meta(document, "id"))
return(document)
}
# wstępne przetwarzanie
corpus <- tm_map(corpus, cut_extension)
corpus <- tm_map(corpus, paste_paragraphs)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
stoplist_file <- "./stopwords_pl.txt"
stoplist <- readLines(
stoplist_file,
encoding = "utf-8"
)
corpus <- tm_map(corpus, removeWords, stoplist)
corpus <- tm_map(corpus, content_transformer(remove_char), intToUtf8(8722))
corpus <- tm_map(corpus, content_transformer(remove_char), intToUtf8(190))
corpus <- tm_map(corpus, content_transformer(trimws))
corpus <- tm_map(corpus, stripWhitespace)
# lematyzacja
polish <- dictionary("pl_PL")
lemmatize <- function(text){
parsed_text_vec <- unlist(hunspell_parse(text, dict = polish))
lemmatized_text_vec <- hunspell_stem(parsed_text_vec, dict = polish)
for (t in 1:length(lemmatized_text_vec)) {
if(length(lemmatized_text_vec[[t]]) == 0) lemmatized_text_vec[t] <- parsed_text_vec[t]
if(length(lemmatized_text_vec[[t]])  > 1) lemmatized_text_vec[t] <- lemmatized_text_vec[[t]][1]
}
lemmatized_text <- paste(lemmatized_text_vec, collapse = " ")
return(lemmatized_text)
}
corpus <- tm_map(corpus, content_transformer(lemmatize))
# eksport przetworzonego korpusu do plików
preprocessed_dir <- "./przetworzone"
dir.create(preprocessed_dir)
writeCorpus(corpus,preprocessed_dir)
corpus_dir <- "./przetworzone"
corpus <- VCorpus(
DirSource(
corpus_dir,
"UTF-8",
"*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
# dodatkowe funkcje transformujące
cut_extension <- function(document){
meta(document, "id") <- gsub("\\.txt$", "", meta(document, "id"))
return(document)
}
# wstępne przetwarzanie
corpus <- tm_map(corpus, cut_extension)
# tworzenie macierzy częstości
tdm_tf_all <- TermDocumentMatrix(corpus)
# transponowana tdm
tdm_tf_all <- TermDocumentMatrix(corpus)
dtm_tf_all <- DocumentTermMatrix(corpus)
dtm_tfidf_all <- DocumentTermMatrix(
corpus,
control = list(
weighting = weightTfIdf
)
)
dtm_tf_bounds <- DocumentTermMatrix(
corpus,
control = list(
bounds = list(
global = c(2,16)
)
)
)
dtm_tfidf_bounds <- DocumentTermMatrix(
corpus,
control = list(
weighting = weightTfIdf,
bounds = list(
global = c(2,16)
)
)
)
tdm_tf_bounds <- TermDocumentMatrix(
corpus,
control = list(
bounds = list(
global = c(2,16)
)
)
)
dtm_tf_all_m <- as.matrix(dtm_tf_all)
dtm_tfidf_all_m <- as.matrix(dtm_tfidf_all)
dtm_tfidf_bounds_m <- as.matrix(dtm_tfidf_bounds)
tdm_tf_bounds_m <- as.matrix(tdm_tf_bounds)
# załadowanie bibliotek
library(topicmodels)
# utworzenie katalogu na wyniki
topics_dir <- "./topics"
dir.create(topics_dir)
# Analiza Ukrytej Alokacji Dirichlet'a (Latent Dirichlet Allocation method)
words_count <- ncol(dtm_tf_all)
topics_count <- 5
lda_model <- LDA(
dtm_tf_all,
topics_count,
method = "Gibbs",
control = list(
burnin = 2000,
thin = 100,
iter = 3000
)
)
results <- posterior(lda_model)
cols = c("lightsteelblue", "orchid", "royalblue", "grey0", "darkolivegreen4", "darkgoldenrod1")
# prezentacja tematów
for (topic_no in 1:topics_count) {
topic_file <- paste(
topics_dir,
paste("Temat", topic_no, ".png"),
sep = "/"
)
png(topic_file)
par(mai = c(1,2,1,1))
topic <- tail(sort(results$terms[topic_no,]),20)
barplot(
topic,
horiz = TRUE,
las = 1,
main = paste("Temat", topic_no),
xlab = "Prawdopodobieństwo",
col = cols[topic_no]
)
dev.off()
}
#prezentacja dokumentów
plot_file <- paste(topics_dir, "Dokumenty.png",sep = "/")
png(plot_file)
par(mai = c(1,4,1,1))
barplot(
t(results$topics),
horiz = TRUE,
las = 1,
main = "Dokumenty",
xlab = "Prawdopodobieństwo",
col = cols
)
dev.off()
# załadowanie bibliotek
library(tm)
library(hunspell)
# utworzenie korpusu dokumentów
corpus_dir <- "./20_Dokumenty"
corpus <- VCorpus(
DirSource(
corpus_dir,
"UTF-8",
"*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
# dodatkowe funkcje transformujące
paste_paragraphs <- content_transformer(
function(text){
paste(text, collapse = " ")
}
)
remove_char <- function(text, char) gsub(char, "", text)
cut_extension <- function(document){
meta(document, "id") <- gsub("\\.txt$", "", meta(document, "id"))
return(document)
}
# wstępne przetwarzanie
corpus <- tm_map(corpus, cut_extension)
corpus <- tm_map(corpus, paste_paragraphs)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
stoplist_file <- "./stopwords_pl.txt"
stoplist <- readLines(
stoplist_file,
encoding = "utf-8"
)
corpus <- tm_map(corpus, removeWords, stoplist)
corpus <- tm_map(corpus, content_transformer(remove_char), intToUtf8(8722))
corpus <- tm_map(corpus, content_transformer(remove_char), intToUtf8(190))
corpus <- tm_map(corpus, content_transformer(trimws))
corpus <- tm_map(corpus, stripWhitespace)
# lematyzacja
polish <- dictionary("pl_PL")
lemmatize <- function(text){
parsed_text_vec <- unlist(hunspell_parse(text, dict = polish))
lemmatized_text_vec <- hunspell_stem(parsed_text_vec, dict = polish)
for (t in 1:length(lemmatized_text_vec)) {
if(length(lemmatized_text_vec[[t]]) == 0) lemmatized_text_vec[t] <- parsed_text_vec[t]
if(length(lemmatized_text_vec[[t]])  > 1) lemmatized_text_vec[t] <- lemmatized_text_vec[[t]][1]
}
lemmatized_text <- paste(lemmatized_text_vec, collapse = " ")
return(lemmatized_text)
}
corpus <- tm_map(corpus, content_transformer(lemmatize))
# eksport przetworzonego korpusu do plików
preprocessed_dir <- "./przetworzone"
dir.create(preprocessed_dir)
writeCorpus(corpus,preprocessed_dir)
corpus_dir <- "./przetworzone"
corpus <- VCorpus(
DirSource(
corpus_dir,
"UTF-8",
"*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
# dodatkowe funkcje transformujące
cut_extension <- function(document){
meta(document, "id") <- gsub("\\.txt$", "", meta(document, "id"))
return(document)
}
# wstępne przetwarzanie
corpus <- tm_map(corpus, cut_extension)
# tworzenie macierzy częstości
tdm_tf_all <- TermDocumentMatrix(corpus)
# transponowana tdm
tdm_tf_all <- TermDocumentMatrix(corpus)
dtm_tf_all <- DocumentTermMatrix(corpus)
dtm_tfidf_all <- DocumentTermMatrix(
corpus,
control = list(
weighting = weightTfIdf
)
)
dtm_tf_bounds <- DocumentTermMatrix(
corpus,
control = list(
bounds = list(
global = c(2,16)
)
)
)
dtm_tfidf_bounds <- DocumentTermMatrix(
corpus,
control = list(
weighting = weightTfIdf,
bounds = list(
global = c(2,16)
)
)
)
tdm_tf_bounds <- TermDocumentMatrix(
corpus,
control = list(
bounds = list(
global = c(2,16)
)
)
)
dtm_tf_all_m <- as.matrix(dtm_tf_all)
dtm_tfidf_all_m <- as.matrix(dtm_tfidf_all)
dtm_tfidf_bounds_m <- as.matrix(dtm_tfidf_bounds)
tdm_tf_bounds_m <- as.matrix(tdm_tf_bounds)
# załadowanie bibliotek
library(topicmodels)
# utworzenie katalogu na wyniki
topics_dir <- "./topics"
dir.create(topics_dir)
# Analiza Ukrytej Alokacji Dirichlet'a (Latent Dirichlet Allocation method)
words_count <- ncol(dtm_tf_all)
topics_count <- 5
lda_model <- LDA(
dtm_tf_all,
topics_count,
method = "Gibbs",
control = list(
burnin = 2000,
thin = 100,
iter = 3000
)
)
results <- posterior(lda_model)
cols = c("lightsteelblue", "orchid", "royalblue", "grey0", "darkolivegreen4", "darkgoldenrod1")
# prezentacja tematów
for (topic_no in 1:topics_count) {
topic_file <- paste(
topics_dir,
paste("Temat", topic_no, ".png"),
sep = "/"
)
png(topic_file)
par(mai = c(1,2,1,1))
topic <- tail(sort(results$terms[topic_no,]),20)
barplot(
topic,
horiz = TRUE,
las = 1,
main = paste("Temat", topic_no),
xlab = "Prawdopodobieństwo",
col = cols[topic_no]
)
dev.off()
}
#prezentacja dokumentów
plot_file <- paste(topics_dir, "Dokumenty.png",sep = "/")
png(plot_file)
par(mai = c(1,4,1,1))
barplot(
t(results$topics),
horiz = TRUE,
las = 1,
main = "Dokumenty",
xlab = "Prawdopodobieństwo",
col = cols
)
dev.off()
# załadowanie bibliotek
library(wordcloud)
# utworzenie katalogu na wyniki
clouds_dir <- "./clouds"
dir.create(clouds_dir)
# waga tf jako miara ważności słów w dokumentach
for (doc_no in 1:length(corpus)) {
print(rownames(dtm_tf_all_m)[doc_no])
print(head(sort(dtm_tf_all_m[doc_no,], decreasing = T)))
}
# waga tfidf jako miara ważności słów w dokumentach
for (doc_no in 1:length(corpus)) {
print(rownames(dtm_tfidf_all_m)[doc_no])
print(head(sort(dtm_tfidf_all_m[doc_no,], decreasing = T)))
}
#chmury tagów
for (doc_no in 1:length(corpus)) {
cloud_file <- paste(
clouds_dir,
paste(corpus[[doc_no]]$meta$id, ".png", sep = ""),
sep = "/"
)
png(cloud_file)
wordcloud(
corpus[doc_no],
max.words = 200,
colors = brewer.pal(8,"Blues")
)
dev.off()
}
